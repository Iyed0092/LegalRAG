# âš–ï¸ LegalRAG - Intelligent Legal Assistant

![CI/CD Pipeline](https://github.com/Iyed0092/LegalRAG/actions/workflows/ci_pipeline.yml/badge.svg)
![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![Docker](https://img.shields.io/badge/Docker-Enabled-blue.svg)
![Status](https://img.shields.io/badge/Status-Prototype-orange.svg)

**LegalRAG** is an advanced Retrieval-Augmented Generation (RAG) system designed to analyze complex legislative documents with high precision.

Unlike standard RAG systems that rely solely on vector similarity, LegalRAG implements a **Hybrid Architecture**:
1.  **Dense Retrieval (ChromaDB):** Finds semantically similar text chunks.
2.  **Knowledge Graph (Neo4j):** Maps and traverses legal dependencies (e.g., "Article 14 refers to Section 3").

This dual approach significantly reduces hallucinations and ensures that answers are grounded in the actual statutory context.

---

## ğŸš€ Key Features

* **ğŸ•¸ï¸ Hybrid Search Engine:** Combines **Vector Embeddings** (Sentence-Transformers) for semantic nuances and **Graph Traversal** (Cypher queries) for structural context.
* **ğŸ›¡ï¸ Hallucination Guardrails:** Implements a strict prompt engineering strategy ("Strict Mode") to prevent the LLM from inventing facts.
* **ğŸ“„ Automated Ingestion:** Robust PDF processing pipeline that extracts text, chunks it intelligently, and indexes it into both databases simultaneously.
* **ğŸ³ Microservices Architecture:** Fully containerized using **Docker Compose** (Django API, Neo4j, ChromaDB, Streamlit).
* **âš¡ Interactive Dashboard:** User-friendly **Streamlit** interface for uploading documents and chatting with the legal assistant.

## ğŸ—ï¸ System Architecture

The system follows a modular ETL (Extract, Transform, Load) and RAG pipeline:

1.  **Ingestion:** PDFs are parsed, cleaned, and split into chunks (LangChain).
2.  **Indexing:**
    * **Vector Store (ChromaDB):** Stores embeddings for similarity search.
    * **Graph Store (Neo4j):** Creates nodes for articles and relationships (`REFERS_TO`) for references.
3.  **Retrieval:** The engine performs a parallel search (Vector + Graph) to gather the most relevant context.
4.  **Generation:** The **FLAN-T5** model synthesizes the answer based *strictly* on the retrieved context.



## ğŸ“‚ Project Structure

```bash
legal_rag_project/
â”œâ”€â”€ .env                       # Environment variables (API Keys, Neo4j/Chroma URLs)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml         # Orchestration (Django + Neo4j + Chroma + Redis)
â”œâ”€â”€ Dockerfile                 # Django Backend Image
â”œâ”€â”€ requirements.txt           # Python Dependencies (Django, DRF, ChromaDB, Neo4j, etc.)
â”œâ”€â”€ manage.py                  # Django CLI
â”‚
â”œâ”€â”€ config/                    # Global Project Config
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ asgi.py
â”‚   â”œâ”€â”€ settings.py            # Project Settings
â”‚   â”œâ”€â”€ urls.py                # Main URL Router
â”‚   â””â”€â”€ wsgi.py
â”‚
â”œâ”€â”€ apps/                      # Functional Modules (Domain Services)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/             # MODULE 1: ETL & Indexing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ apps.py
â”‚   â”‚   â”œâ”€â”€ models.py          # SQL Model for file tracking (DocumentMetadata)
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_parser.py  # Text Extraction (PyPDF/LangChain)
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py     # Semantic Chunking
â”‚   â”‚   â”‚   â””â”€â”€ loader.py      # Orchestrator sending data to Chroma & Neo4j
â”‚   â”‚   â””â”€â”€ views.py           # File Upload API
â”‚   â”‚
â”‚   â”œâ”€â”€ rag_engine/            # MODULE 2: The Brain (Retrieval + LLM)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ apps.py
â”‚   â”‚   â”œâ”€â”€ connectors/        # Database Connectors
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py # ChromaDB Wrapper + SentenceTransformers
â”‚   â”‚   â”‚   â””â”€â”€ graph_store.py  # Neo4j Wrapper (Cypher queries)
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â””â”€â”€ flan_t5.py      # Inference Logic (HuggingFace Pipeline)
â”‚   â”‚   â”œâ”€â”€ logic/
â”‚   â”‚   â”‚   â””â”€â”€ hybrid_search.py # Hybrid Fusion Logic (Vector + Graph)
â”‚   â”‚   â””â”€â”€ views.py            # Q&A API (/ask)
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/            # MODULE 3: Benchmarking (Recall@5)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ scripts/
â”‚       â”‚   â”œâ”€â”€ generate_ground_truth.py
â”‚       â”‚   â””â”€â”€ calculate_recall.py
â”‚       â””â”€â”€ metrics.py
â”‚
â””â”€â”€ data/                      # Local Storage (Git Ignored)
    â”œâ”€â”€ raw_pdfs/              # Raw legislative PDFs
    â””â”€â”€ chroma_db/             # ChromaDB Persistence


```

## ğŸ› ï¸ Installation & Getting Started

### Prerequisites
* **Docker Desktop** (running)
* **Python 3.10+** (for local model downloading)
* **Git**

### 1. Clone the Repository
```bash
git clone [https://github.com/Iyed0092/LegalRAG.git](https://github.com/Iyed0092/LegalRAG.git)
cd LegalRAG
```

2. Download AI Models (Important)
Since Large Language Models (LLMs) are too heavy for GitHub, you need to download them locally first. A script is provided to automate this:

```bash
# Install utils for the download script
pip install transformers torch sentence-transformers

# Run the downloader
python download_models.py
```
This will fetch FLAN-T5 and MiniLM and place them in the data/ai_models/ directory, which is mounted into the Docker containers.

3. Run with Docker
Once models are ready, launch the full stack:

```bash
docker-compose up --build
```
ğŸ–¥ï¸ Usage
User Interface (Streamlit): Open http://localhost:8501

Backend API (Django): Open http://localhost:8000/api/v1/

Neo4j Browser: Open http://localhost:7474 (User: neo4j, Password: password)

## ğŸ”§ Tech Stack

| Category | Technologies |
| :--- | :--- |
| **Backend** | Python, Django REST Framework |
| **AI / ML** | PyTorch, HuggingFace Transformers, LangChain, FLAN-T5 |
| **Databases** | Neo4j (Graph), ChromaDB (Vector) |
| **Frontend** | Streamlit |
| **DevOps** | Docker, Docker Compose, GitHub Actions (CI/CD) |

## ğŸ“œ License

This project is intended for educational purposes as a personal project done by me.
Distributed under the **MIT License**.

## ğŸ‘¤ Author

**Iyed Mekki**
* **LinkedIn:** [linkedin.com/in/iyed-mekki](https://www.linkedin.com/in/iyed-mekki-265002384/)
* **GitHub:** [github.com/Iyed0092](https://github.com/Iyed0092)
















